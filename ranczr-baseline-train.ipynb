{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import albumentations as a_transform\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import AverageMeter\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# WORKDIR = os.path.expanduser(\"~/project/data/ranzcr-clip-catheter-line-classification\")\n",
    "WORKDIR = \"../data/ranczr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(os.path.join(WORKDIR, \"train.csv\"))\n",
    "train_annot = pd.read_csv(os.path.join(WORKDIR, \"train_annotations.csv\"))\n",
    "weird_uid = '1.2.826.0.1.3680043.8.498.93345761486297843389996628528592497280'\n",
    "train_csv.loc[train_csv.StudyInstanceUID==weird_uid, 'ETT - Abnormal'] = 0\n",
    "train_csv.loc[train_csv.StudyInstanceUID==weird_uid, 'CVC - Abnormal'] = 1\n",
    "train_annot.loc[4344, 'label'] = 'CVC - Abnormal'\n",
    "# tracheal_point = pd.read_csv(os.path.join(WORKDIR, \"tracheal_bifurcation.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    debug=False\n",
    "    print_freq=100\n",
    "    num_workers=4\n",
    "    model_name='resnext50_32x4d'\n",
    "    size=256\n",
    "    scheduler='CosineAnnealingLR'\n",
    "    epochs=5\n",
    "    T_max=6 \n",
    "    lr=0.001\n",
    "    min_lr=0.000001\n",
    "    batch_size=16\n",
    "    weight_decay=1e-6\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    seed=1234\n",
    "    target_size=11\n",
    "    target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal',\n",
    "                 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', \n",
    "                 'NGT - Normal', \n",
    "                 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n",
    "                 'Swan Ganz Catheter Present']\n",
    "    n_fold=4\n",
    "    trn_fold=[0, 1, 2, 3]\n",
    "    train=True\n",
    "    \n",
    "# if CFG.debug:\n",
    "#     CFG.epochs = 1\n",
    "#     train = train.sample(n=100, random_state=CFG.seed).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV split creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    7521\n",
       "1    7521\n",
       "2    7521\n",
       "3    7520\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folds = train_csv.copy()\n",
    "Fold = GroupKFold(n_splits=CFG.n_fold)\n",
    "groups = folds['PatientID'].values\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(folds, folds[CFG.target_cols], groups)):\n",
    "    folds.loc[val_index, 'fold'] = int(n)\n",
    "folds['fold'] = folds['fold'].astype(int)\n",
    "display(folds.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset, DataLoader and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = a_transform.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225], p=1.0, max_pixel_value=255.0)\n",
    "\n",
    "train_transform = a_transform.Compose([a_transform.RandomResizedCrop(256, 256, scale=(0.85,1.), p=1.0),\n",
    "                                       a_transform.HorizontalFlip(p=0.5),\n",
    "                                       normalize,\n",
    "                                       ToTensorV2()\n",
    "                                       ], p=1.0)\n",
    "\n",
    "valid_transform = a_transform.Compose([a_transform.Resize(256, 256), normalize, ToTensorV2()], p=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import TrainDataset, SegDataset, AnnotDataset\n",
    "\n",
    "# Select fold - loop curfold\n",
    "curfold = 0\n",
    "trn_idx = folds[folds['fold'] != curfold].index\n",
    "val_idx = folds[folds['fold'] == curfold].index\n",
    "\n",
    "train_folds = folds.loc[trn_idx].reset_index(drop=True)\n",
    "valid_folds = folds.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "train_folds = train_folds[train_folds['StudyInstanceUID'].isin(train_annot['StudyInstanceUID'].unique())].reset_index(drop=True)\n",
    "valid_folds = valid_folds[valid_folds['StudyInstanceUID'].isin(train_annot['StudyInstanceUID'].unique())].reset_index(drop=True)\n",
    "\n",
    "valid_labels = valid_folds[CFG.target_cols].values\n",
    "\n",
    "# train_uid = [x for x in train_folds[\"StudyInstanceUID\"].values]\n",
    "# valid_uid = [x for x in valid_folds[\"StudyInstanceUID\"].values]\n",
    "# Initialize train and valid dataset\n",
    "train_dataset = AnnotDataset(WORKDIR, train_folds, train_annot,\n",
    "                             flip_transform=train_transform, \n",
    "                             target_cols=CFG.target_cols)\n",
    "valid_dataset = AnnotDataset(WORKDIR, valid_folds, train_annot,\n",
    "                             flip_transform=valid_transform, \n",
    "                             target_cols=CFG.target_cols)\n",
    "\n",
    "# Initialize dataloader for fast parsing\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, \n",
    "                          batch_size=CFG.batch_size, \n",
    "                          shuffle=False, \n",
    "                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea\n",
    "\n",
    "## 1. Lung Segmentation map\n",
    "\n",
    "Train Pix2Pix for lung segmentation. Dynamically crop the bbox and focus only on the cropped image.\n",
    "\n",
    "## 2. Relative Position\n",
    "\n",
    "Model the relative position of the catheters to lungs. Think heatmapping.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "from fastai.vision.learner import create_body\n",
    "from torchvision.models.resnet import resnet18\n",
    "from fastai.vision.models.unet import DynamicUnet\n",
    "\n",
    "\n",
    "body = create_body(resnet18, pretrained=True, n_in=3, cut=-2)\n",
    "netG = DynamicUnet(body, 1, (256, 256)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_c, num_filters=64, n_down=3):\n",
    "        super().__init__()\n",
    "        model = [self.get_layers(input_c, num_filters, norm=False)]\n",
    "        model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down-1) else 2) \n",
    "                          for i in range(n_down)] # the 'if' statement is taking care of not using\n",
    "                                                  # stride of 2 for the last block in this loop\n",
    "        model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)] # Make sure to not use normalization or\n",
    "                                                                                             # activation for the last layer of the model\n",
    "        self.model = nn.Sequential(*model)                                                   \n",
    "        \n",
    "    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True): # when needing to make some repeatitive blocks of layers,\n",
    "        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]          # it's always helpful to make a separate method for that purpose\n",
    "        if norm: layers += [nn.BatchNorm2d(nf)]\n",
    "        if act: layers += [nn.LeakyReLU(0.2, True)]\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# input channel is 4 because we its conditional GAN\n",
    "netD = PatchDiscriminator(4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        if use_lsgan:\n",
    "            self.loss = nn.MSELoss()\n",
    "        else:\n",
    "            # self.loss = nn.BCELoss()\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def get_target_tensor(self, in_vec, target_is_real):\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(in_vec)\n",
    "\n",
    "    def __call__(self, in_vec, target_is_real):\n",
    "        target_tensor = self.get_target_tensor(in_vec, target_is_real)\n",
    "        return self.loss(in_vec, target_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop for the Unet-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optG = torch.optim.Adam(netG.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "optD = torch.optim.Adam(netD.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "# CFG.T_max = int(CFG.epochs * len(train_loader))\n",
    "\n",
    "schedulerG = torch.optim.lr_scheduler.CosineAnnealingLR(optG, T_max=CFG.T_max, eta_min=CFG.min_lr)\n",
    "schedulerD = torch.optim.lr_scheduler.CosineAnnealingLR(optD, T_max=CFG.T_max, eta_min=CFG.min_lr)\n",
    "\n",
    "criterionGAN = GANLoss(False).to(device)\n",
    "criterionL1 = nn.L1Loss()\n",
    "criterion_recon = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epc in range(CFG.epochs):\n",
    "\n",
    "    for img_mb, mask_mb, _ in train_loader:\n",
    "        img_mb = img_mb.to(device)\n",
    "        # mask_mb = mask_mb.to(device)\n",
    "\n",
    "        #### Train netD ####\n",
    "        optD.zero_grad()\n",
    "\n",
    "        # train with fake\n",
    "        fake_mask = netG(img_mb)\n",
    "        fake_ab = torch.cat((img_mb, fake_mask), dim=1)\n",
    "        fake_D = netD(fake_ab.detach())\n",
    "        lossD_fake = criterionGAN(fake_D, False)\n",
    "\n",
    "        # train with real\n",
    "        real_ab = torch.cat((img_mb, mask_mb[0].to(device)), dim=1)\n",
    "        real_D = netD(real_ab)\n",
    "        lossD_real = criterionGAN(real_D, True)\n",
    "\n",
    "        lossD = (lossD_fake + lossD_real) * 0.5\n",
    "        lossD.backward()\n",
    "        optD.step()\n",
    "        \n",
    "        #### Train netG ####\n",
    "        optG.zero_grad()\n",
    "\n",
    "        # First, G(A) should fake the discriminator\n",
    "        fake_ab = torch.cat((img_mb, fake_mask), dim=1)\n",
    "        fake_D = netD(fake_ab)\n",
    "        lossG_gan = criterionGAN(fake_D, True)\n",
    "\n",
    "        # Second, G(A) = B\n",
    "        lossG_l1 = criterionL1(fake_mask, mask_mb[0].to(device)) * 0.1\n",
    "        lossG = lossG_gan + lossG_l1\n",
    "        \n",
    "        lossG.backward()\n",
    "        optG.step()\n",
    "    schedulerG.step()\n",
    "    schedulerD.step()\n",
    "    avg_psnr = 0\n",
    "    # TODO: fix the reporting metric\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            img_mb, mask_mb = batch[0].to(device), batch[1][0].to(device)\n",
    "\n",
    "            prediction = netG(img_mb)\n",
    "            mse = criterion_recon(prediction, mask_mb)\n",
    "            # psnr = 10 * log10(1 / mse.item())\n",
    "            avg_psnr += mse\n",
    "        print(f\"===> Epoch [{epc+1}] Avg. PSNR: {avg_psnr / len(valid_loader):.4f} dB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Smaller U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet\n",
    "\n",
    "# net = UNet(n_channels=3, n_classes=1, bilinear=True).to(device)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr)\n",
    "\n",
    "# criterion_recon = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = np.inf\n",
    "for epc in range(CFG.epochs):\n",
    "    losses = AverageMeter()\n",
    "    # scores = AverageMeter()\n",
    "    for step, (img_mb, mask_mb, _) in enumerate(train_loader):\n",
    "        img_mb = img_mb.to(device)\n",
    "\n",
    "        # Model prediction\n",
    "        mask_pred = net(img_mb)\n",
    "        recon_loss = criterion_recon(mask_pred, mask_mb[0].to(device))\n",
    "        batch_size = img_mb.size(0)\n",
    "        # Record Loss\n",
    "        losses.update(recon_loss.item(), batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        recon_loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(net.parameters(), 1000)\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0 or step == (len(train_loader)-1):\n",
    "            print_str = f'Epoch: [{epc+1}][{step}/{len(train_loader)}] '\\\n",
    "            f'Loss: {losses.val:.4f}({losses.avg:.4f}) '\\\n",
    "            f'Grad: {grad_norm:.4f}'\n",
    "            print(print_str)\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_valid = AverageMeter()\n",
    "    # TODO: fix the reporting metric\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            img_mb, mask_mb = batch[0].to(device), batch[1][0].to(device)\n",
    "            prediction = net(img_mb)\n",
    "            mse = criterion_recon(prediction, mask_mb)\n",
    "            avg_valid.update(mse.item(), img_mb.size(0))\n",
    "        print(f\"===> Epoch [{epc+1}] {avg_valid.val:.4f}({avg_valid.avg:.4f})\")\n",
    "        if avg_valid.avg < best_loss:\n",
    "            best_loss = avg_valid.avg\n",
    "            print(f'Epoch {epc+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "            torch.save({'model': net.state_dict()},\n",
    "                        f'seg-fcn8_best.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Trained Unet Segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (inc): DoubleConv(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (down1): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): Down(\n",
       "    (maxpool_conv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): DoubleConv(\n",
       "        (double_conv): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up4): Up(\n",
       "    (up): Upsample(scale_factor=2.0, mode=bilinear)\n",
       "    (conv): DoubleConv(\n",
       "      (double_conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc): OutConv(\n",
       "    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tube_net = UNet(n_channels=3, n_classes=1, bilinear=True).to(device)\n",
    "checkpoint = torch.load(\"tube-seg-unet_best.pth\")\n",
    "tube_net.load_state_dict(checkpoint[\"model\"])\n",
    "tube_net.eval()\n",
    "\n",
    "lung_net = UNet(n_channels=3, n_classes=1, bilinear=True).to(device)\n",
    "checkpoint = torch.load(\"lung-seg-unet_best.pth\")\n",
    "lung_net.load_state_dict(checkpoint[\"model\"])\n",
    "lung_net.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Checking Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidDataset(Dataset):\n",
    "    def __init__(self, root, df, transform=None, target_cols=None):\n",
    "        self.root = root\n",
    "        self.df = df\n",
    "        self.img_idx = df[\"StudyInstanceUID\"].values\n",
    "        self.labels = torch.from_numpy(df[target_cols].values).float()\n",
    "        self.transform = transform\n",
    "        self.train_path = os.path.join(self.root, \"train\")\n",
    "        # self.clahe = cv2.createCLAHE(clipLimit=30.0, tileGridSize=(8, 8))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.img_idx[idx]\n",
    "        file_path = f\"{self.train_path}/{file_name}.jpg\"\n",
    "        image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image_size = image.shape\n",
    "        # image = self.clahe.apply(image)\n",
    "        # image = reshape_img(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        return image, file_name, image_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual check\n",
    "\n",
    "# net.eval()\n",
    "idx = 100\n",
    "an_img = valid_dataset[idx][0]\n",
    "img = cv2.imread(os.path.join(valid_dataset.img_path, f'{valid_dataset.file_names[idx]}.jpg'))\n",
    "pred_lung_mask = nn.functional.sigmoid(lung_net(an_img.unsqueeze(0).to(device))).detach().cpu().squeeze(0).squeeze(0).numpy()\n",
    "pred_tube_mask = nn.functional.sigmoid(tube_net(an_img.unsqueeze(0).to(device))).detach().cpu().squeeze(0).squeeze(0).numpy()\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(an_img.permute(1,2,0).numpy(),'gray')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(pred_tube_mask,\"gray\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(np.round(pred_lung_mask),\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "ds = ValidDataset(WORKDIR, train_csv, valid_transform, CFG.target_cols)\n",
    "ds_loader = DataLoader(ds, batch_size=1, shuffle=False, drop_last=False, num_workers=1)\n",
    "\n",
    "for idx, (img_mb, name_mb, size_mb) in enumerate(ds_loader):\n",
    "    pdb.set_trace()\n",
    "    img_mb = img_mb.to(device)\n",
    "    tube_mask = nn.functional.sigmoid(tube_net(img_mb)).detach().cpu().squeeze(0).squeeze(0).numpy()\n",
    "    tube_mask = tube_mask.cpu().detach().numpy()\n",
    "    tube_mask = cv2.resize(tube_mask, size_mb)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img_paths = glob.glob(os.path.join(WORKDIR,'test/*.jpg'))\n",
    "image = cv2.imread(test_img_paths[10],-1)\n",
    "aug_img = valid_transform(image=image)\n",
    "aug_img = normalize_transform(image=aug_img[\"image\"])\n",
    "image = aug_img[\"image\"]\n",
    "img_mask = netG(image.unsqueeze(0).to(device))\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img_mask.cpu().detach().squeeze(0).squeeze(0).numpy(),\"gray\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(image.numpy().squeeze(0),\"gray\")\n",
    "\n",
    "# test_img_paths[10].split(\"/\")[-1][:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annot[train_annot.StudyInstanceUID == uid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = train_annot.StudyInstanceUID.values[9485]\n",
    "img = cv2.imread(os.path.join(WORKDIR, f\"train/{uid}.jpg\"), -1)\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "mask_img = np.zeros_like(img)\n",
    "# blank_img = np.zeros_like(img)\n",
    "ctr_lines = train_annot[train_annot.StudyInstanceUID == uid].data.values\n",
    "ctr_labels = train_annot[train_annot.StudyInstanceUID == uid].label.values\n",
    "print(ctr_labels)\n",
    "ctr_color = {\"ETT\": (255,0,0), \"CVC\": (255,0,0), \"NGT\": (255,0,0)}\n",
    "for each_ctr, each_label in zip(ctr_lines,ctr_labels):\n",
    "    ctr_cord = ast.literal_eval(each_ctr)\n",
    "    mask_img = cv2.polylines(mask_img, np.array([[np.array(x) for x in ctr_cord]]), isClosed=False, color=ctr_color[each_label[:3]], thickness=20)\n",
    "# ctr_1 = ast.literal_eval(train_annot[train_annot.StudyInstanceUID == uid].loc[17523,\"data\"])\n",
    "# ctr_2 = ast.literal_eval(train_annot[train_annot.StudyInstanceUID == uid].loc[17521,\"data\"])\n",
    "# img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "img[:,:,2] = mask_img\n",
    "# blank_img = cv2.polylines(blank_img, np.array([[np.array(x) for x in ctr_1]]), isClosed=False, color=(255,), thickness=20)\n",
    "# blank_img = cv2.polylines(blank_img, np.array([[np.array(x) for x in ctr_2]]), isClosed=False, color=(255,), thickness=20)\n",
    "# img = cv2.polylines(img, np.array([[np.array(x) for x in ctr_2]]), isClosed=False, color=(0,255), thickness=20)\n",
    "# img = cv2.drawContours(img, np.array([[np.array(x) for x in ctr_2]]), 0, (255), 5)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img,\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = train_csv.StudyInstanceUID.values[1]\n",
    "img = cv2.imread(os.path.join(WORKDIR,f\"cropped_train/{uid}.jpg\"),cv2.IMREAD_GRAYSCALE)\n",
    "clahe = cv2.createCLAHE(clipLimit=30.0, tileGridSize=(8,8))\n",
    "cl_img = clahe.apply(img)\n",
    "\n",
    "ht, wd = cl_img.shape\n",
    "ww = max(ht+2,wd+2)\n",
    "hh = ww\n",
    "constant = np.zeros((hh,ww), dtype=np.uint8)\n",
    "xx = (ww-wd)//2\n",
    "yy = (hh-ht)//2\n",
    "constant[yy:yy+ht, xx:xx+wd] = cl_img\n",
    "\n",
    "target_area = 1024*1024\n",
    "ratio = float(constant.shape[1])/float(constant.shape[0])\n",
    "new_h = int(np.sqrt(target_area / ratio) + 0.5)\n",
    "new_w = int((new_h * ratio) + 0.5)\n",
    "\n",
    "res_img = cv2.resize(constant, (new_w,new_h))\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img,'gray')\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(cl_img,'gray')\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(res_img,'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Tube Segmentation Mask\n",
    "for idx, arow in train_annot.iterrows():\n",
    "    if (idx+1) % 500 == 0 or idx == train_annot.shape[0]-1:\n",
    "        print(f\"Processing {idx+1}\")\n",
    "    uid = arow[0]\n",
    "    tube_type = arow[1][:3]\n",
    "    annot = ast.literal_eval(arow[-1])\n",
    "    img = cv2.imread(os.path.join(WORKDIR,f\"train/{uid}.jpg\"),cv2.IMREAD_GRAYSCALE)\n",
    "    mask_img = np.zeros_like(img)\n",
    "    mask_img = cv2.polylines(mask_img, np.array([[np.array(x) for x in annot]]), isClosed=False, color=(255,), thickness=5)\n",
    "\n",
    "    ii = 0\n",
    "    file_name = os.path.join(WORKDIR,f\"cat_masks/{uid}-{tube_type}-{ii}.jpg\")\n",
    "    while os.path.exists(file_name):\n",
    "        ii += 1\n",
    "        file_name = os.path.join(WORKDIR,f\"cat_masks/{uid}-{tube_type}-{ii}.jpg\")\n",
    "\n",
    "    cv2.imwrite(file_name, mask_img)\n",
    "    # if idx == 10:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CLAHE'd image\n",
    "clahe = cv2.createCLAHE(clipLimit=30.0, tileGridSize=(8,8))\n",
    "\n",
    "for idx, arow in train_csv.iterrows():\n",
    "    if idx == train_annot.shape[0]-1:\n",
    "        print(f\"Processing {idx+1}\")\n",
    "    uid = arow[0]\n",
    "    img = cv2.imread(os.path.join(WORKDIR,f\"train/{uid}.jpg\"),cv2.IMREAD_GRAYSCALE)\n",
    "    cl_img = clahe.apply(img)\n",
    "    \n",
    "    # zero pad\n",
    "    ht, wd = cl_img.shape\n",
    "    ww = max(ht+2,wd+2)\n",
    "    hh = ww\n",
    "    constant = np.zeros((hh,ww), dtype=np.uint8)\n",
    "    xx = (ww-wd)//2\n",
    "    yy = (hh-ht)//2\n",
    "    constant[yy:yy+ht, xx:xx+wd] = cl_img\n",
    "\n",
    "    target_area = 1024*1024\n",
    "    ratio = float(constant.shape[1])/float(constant.shape[0])\n",
    "    new_h = int(np.sqrt(target_area / ratio) + 0.5)\n",
    "    new_w = int((new_h * ratio) + 0.5)\n",
    "\n",
    "    res_img = cv2.resize(constant, (new_w,new_h))\n",
    "    file_name = os.path.join(WORKDIR,f\"preprocessed_train/{uid}.jpg\")\n",
    "    cv2.imwrite(file_name, res_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnms(proposal_bboxs):\n",
    "    abox = proposal_bboxs[0].bbox\n",
    "    min_row = abox[0]\n",
    "    min_col = abox[1]\n",
    "    max_row = abox[2]\n",
    "    max_col = abox[3]\n",
    "    # (min_row, min_col, max_row, max_col)\n",
    "    for prop in proposal_bboxs:\n",
    "        abox = prop.bbox\n",
    "        if abox[0] < min_row or abox[1] < min_col or abox[2] > max_row or abox[3] > max_col:\n",
    "            min_row = abox[0]\n",
    "            min_col = abox[1]\n",
    "            max_row = abox[2]\n",
    "            max_col = abox[3]\n",
    "        # if abox[0] < min_row:\n",
    "        #     min_row = abox[0]\n",
    "        # if abox[1] < min_col:\n",
    "        #     min_col = abox[1]\n",
    "        # if abox[2] > max_row:\n",
    "        #     max_row = abox[2]\n",
    "        # if abox[3] > max_col:\n",
    "        #     max_col = abox[3]\n",
    "    return (min_row, min_col, max_row, max_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import label, regionprops\n",
    "\n",
    "mask_paths = glob.glob(os.path.join(WORKDIR,\"train_lung_masks/*.jpg\"))\n",
    "uid = mask_paths[198].split(\"/\")[-1][:-4]\n",
    "img = cv2.imread(os.path.join(WORKDIR,f\"train/{uid}.jpg\"))\n",
    "img_mask = cv2.imread(os.path.join(WORKDIR,f\"train_lung_masks/{uid}.jpg\"), -1)\n",
    "# lbl0 = label(img_mask)\n",
    "props = regionprops(img_mask)\n",
    "# write function here\n",
    "nms_box = cnms(props)\n",
    "# for prop in props:\n",
    "    # print(prop.bbox)\n",
    "cv2.rectangle(img, (nms_box[1], nms_box[0]), (nms_box[3], nms_box[2]), (255, 0, 0), 2)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img,'gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(img[nms_box[0]:nms_box[2], nms_box[1]:nms_box[3]], 'gray')\n",
    "plt.show()\n",
    "new_img = img[nms_box[0]:nms_box[2], nms_box[1]:nms_box[3]]\n",
    "cv2.imwrite(os.path.join(WORKDIR,f\"cropped_train/{uid}.jpg\"),new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN/DELETE\n",
    "\n",
    "# for apath in mask_paths:\n",
    "#     uid = apath.split(\"/\")[-1][:-4]\n",
    "#     img = cv2.imread(os.path.join(WORKDIR,f\"train/{uid}.jpg\"))\n",
    "#     img_mask = cv2.imread(os.path.join(WORKDIR,f\"train_lung_masks/{uid}.jpg\"), -1)\n",
    "#     props = regionprops(img_mask)\n",
    "#     nms_box = cnms(props)\n",
    "#     new_img = img[nms_box[0]:nms_box[2], nms_box[1]:nms_box[3]]\n",
    "#     cv2.imwrite(os.path.join(WORKDIR,f\"cropped_train/{uid}.jpg\"),new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid='1.2.826.0.1.3680043.8.498.10000428974990117276582711948006105617'\n",
    "img = cv2.imread(os.path.join(WORKDIR,f\"train_lung_masks/{uid}.jpg\"))\n",
    "plt.imshow(img,'gray')\n",
    "# img.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "pydev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
